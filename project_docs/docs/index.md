# Project Name

## Github

- [Github](./Learn_Github/index.md)

---

## LICENSE

- [LICENSE](Learn_License/index.md)

---

## Vite Press

- [Vite Press](Learn_Vite_Press/index.md)

---

## Django

- [Django](Learn_Django/index.md)

### ğŸ”§ Django

```cmd
pip install python-decouple
```

```cmd
pip install lxml
```

```cmd
pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
```

```cmd
pip install playwright celery redis
```

```cmd
playwright install
```

```cmd
pip install playwright
```

```cmd
pip install gspread oauth2client
```

```cmd
pip install rich
```

### ğŸ•¸ï¸ scraper

#### ğŸ“ [ models.py ]

- ÙŠÙ…Ø«Ù„ Ù…ÙˆÙ‚Ø¹ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙˆØ§Ø­Ø¯ Ù‡ØªØ³Ø­Ø¨ Ù…Ù†Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

- ÙƒÙ„ Ù…ÙˆÙ‚Ø¹ Ù„ÙŠÙ‡ Ø§Ø³Ù… + Ø±Ø§Ø¨Ø· Ø±Ø¦ÙŠØ³ÙŠ + ÙˆØµÙ + Ø­Ø§Ù„Ø© Ø§Ù„ØªÙØ¹ÙŠÙ„.

```python
from django.db import models
from django.contrib.auth.models import User
import json

class Website(models.Model):
    name = models.CharField(max_length=200)
    base_url = models.URLField()
    description = models.TextField(blank=True)
    is_active = models.BooleanField(default=True)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

    def __str__(self):
        return self.name

```

- Ø¨ÙŠÙ…Ø«Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ø­Ø¨ (scraping) Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¹ÙŠÙ†.

- Ø¨ÙŠÙ‚ÙˆÙ„Ùƒ Ù‡ØªØ³Ø­Ø¨ Ø¥ÙŠÙ‡ØŒ Ùˆ Ù…Ù†ÙŠÙ†ØŒ Ùˆ Ø¥Ø²Ø§ÙŠ.

- Meta ÙŠØ¹Ù†ÙŠ Ù…Ø§ ÙŠÙ†ÙØ¹Ø´ ÙŠÙƒÙˆÙ† ÙÙŠ Ù…ÙˆÙ‚Ø¹ Ø¹Ù†Ø¯Ù‡ Ù†ÙØ³ field_name Ù…Ø±ØªÙŠÙ†.

```python
class ScrapingConfig(models.Model):
    SELECTOR_TYPES = [
        ('css', 'CSS Selector'),
        ('xpath', 'XPath'),
    ]

    website = models.ForeignKey(
        Website, on_delete=models.CASCADE, related_name='configs')
    # e.g., 'product_name', 'price', 'description'
    field_name = models.CharField(max_length=100)
    selector = models.TextField()  # The actual CSS selector or XPath
    selector_type = models.CharField(
        max_length=10, choices=SELECTOR_TYPES, default='css')
    is_required = models.BooleanField(default=True)
    # text, number, url, etc.
    data_type = models.CharField(max_length=50, default='text')

    class Meta:
        unique_together = ['website', 'field_name']

    def __str__(self):
        return f"{self.website.name} - {self.field_name}"

```

- ÙƒÙ„ Ù…Ù‡Ù…Ø© (Job) Ø¨ØªÙ…Ø«Ù„ Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ù…Ø¹ÙŠÙ†.

- Ø¨ØªØ±ØµØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (pending - running - completed - failed)

```python
class ScrapingJob(models.Model):
    STATUS_CHOICES = [
        ('pending', 'Pending'),
        ('running', 'Running'),
        ('completed', 'Completed'),
        ('failed', 'Failed'),
    ]

    website = models.ForeignKey(Website, on_delete=models.CASCADE)
    target_url = models.URLField()
    status = models.CharField(
        max_length=20, choices=STATUS_CHOICES, default='pending')
    started_at = models.DateTimeField(auto_now_add=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    error_message = models.TextField(blank=True)
    items_scraped = models.IntegerField(default=0)

    def __str__(self):
        return f"{self.website.name} - {self.started_at.strftime('%Y-%m-%d %H:%M')}"

```

- Ø¨ÙŠÙ…Ø«Ù„ Ø¹Ù†ØµØ± Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ… Ø³Ø­Ø¨Ù‡ ÙÙŠ Ù†ØªÙŠØ¬Ø© Job Ù…Ø¹ÙŠÙ†Ø©.

- Ø¨ÙŠØªØ®Ø²Ù† ÙƒÙ€ JSON.

```python
class ScrapedData(models.Model):
    job = models.ForeignKey(
        ScrapingJob, on_delete=models.CASCADE, related_name='scraped_items')
    data = models.JSONField()  # Stores the actual scraped data as JSON
    scraped_at = models.DateTimeField(auto_now_add=True)
    source_url = models.URLField()  # The specific page this data came from

    def __str__(self):
        return f"Data from {self.job.website.name} - {self.scraped_at}"

```

#### ğŸ“ [ views.py ]

- API Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Website) â€“ ØªÙ‚Ø¯Ø± ØªØ¹Ù…Ù„:

- Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ‚Ø¹

- ØªØ¹Ø¯ÙŠÙ„

- Ø­Ø°Ù

- Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„

- Ø¹Ø±Ø¶ ÙˆØ§Ø­Ø¯

```python
class WebsiteViewSet(viewsets.ModelViewSet):
    queryset = Website.objects.all().order_by(
        '-created_at')

    serializer_class = WebsiteSerializer

    @action(detail=True, methods=['get'])
    def configs(self, request, pk=None):
        """Get all scraping configurations for a website"""
        website = self.get_object()
        configs = website.configs.all()
        serializer = ScrapingConfigSerializer(configs, many=True)
        return Response(serializer.data)

    @action(detail=True, methods=['get'])
    def jobs(self, request, pk=None):
        """Get all scraping jobs for a website"""
        website = self.get_object()
        jobs = website.scrapingjob_set.all().order_by('-started_at')
        serializer = ScrapingJobSerializer(jobs, many=True)
        return Response(serializer.data)

```

- Ø¨ÙŠØ±Ø¬Ø¹ ÙƒÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø³Ø­Ø¨ (ScrapingConfig) Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¯Ù‡.

```python
class ScrapingConfigViewSet(viewsets.ModelViewSet):
    serializer_class = ScrapingConfigSerializer

    def get_queryset(self):
        website_id = self.request.query_params.get('website', None)
        if website_id:
            return ScrapingConfig.objects.filter(website_id=website_id)
        return ScrapingConfig.objects.all()

```

- Ø¨ÙŠØ±Ø¬Ø¹ ÙƒÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„Ø³Ø­Ø¨ (ScrapingJob) Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¯Ù‡.

```python

class ScrapingJobViewSet(viewsets.ModelViewSet):
    queryset = ScrapingJob.objects.all().order_by('-started_at')
    serializer_class = ScrapingJobSerializer

    @action(detail=False, methods=['post'])
    def start_scraping(self, request):
        """Start a new scraping job"""
        website_id = request.data.get('website_id')
        target_url = request.data.get('target_url')

        if not website_id or not target_url:
            return Response(
                {'error': 'website_id and target_url are required'},
                status=status.HTTP_400_BAD_REQUEST
            )

        website = get_object_or_404(Website, id=website_id)

        # Create the job
        job = ScrapingJob.objects.create(
            website=website,
            target_url=target_url,
            status='pending'
        )

        # TODO: In Step 4, we'll add the actual scraping logic here
        # For now, just return the created job
        serializer = ScrapingJobSerializer(job)
        return Response(serializer.data, status=status.HTTP_201_CREATED)

    @action(detail=True, methods=['get'])
    def results(self, request, pk=None):
        """Get scraped data for a specific job"""
        job = self.get_object()
        scraped_data = job.scraped_items.all()
        serializer = ScrapedDataSerializer(scraped_data, many=True)
        return Response(serializer.data)

```

---

---

---

---
